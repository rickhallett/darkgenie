Back to [README](../README.md)

## Product Requirements Document: Dark Genie (MVP)

**1. Introduction & Purpose**

*   **Product Name:** Dark Genie
*   **Purpose:** To provide a voice-activated, AI-driven deep research engine that programmatically orchestrates multiple research instances, verifies the output for accuracy, and stores comprehensive findings in the user's Google Drive.
*   **Problem Solved:** Automates time-consuming deep research, provides programmatic access to advanced iterative AI research capabilities, enhances the reliability of AI-generated research, and streamlines information management.
*   **Target User:** An individual requiring in-depth, cited research on various topics, who values automation and organized digital information.

**2. Goals (MVP)**

1.  Enable users to initiate deep research on a specific query via voice command.
2.  Programmatically leverage Anthropic's Claude API (with web search tool) to conduct iterative, multi-step research based on the user's query and defined parameters (depth/scope).
3.  Implement an AI-driven verification layer to assess the accuracy and identify potential hallucinations in the initial research reports.
4.  Automatically aggregate, name, and store the verified (or annotated) research findings into the user's Google Drive.
5.  Establish a foundational, extensible architecture for future enhancements.

**3. Scope (MVP)**

*   **In Scope:**
    *   Basic voice input capture.
    *   Parsing of voice input to extract research query, depth parameters (e.g., number of "agents"/iterations), and goal.
    *   LLM-based optimization of the user query for Claude's deep research tool.
    *   Programmatic invocation of Claude's API with the `web_search` tool for research generation.
    *   Programmatic invocation of a secondary "Evaluator LLM" for report verification.
    *   Basic aggregation and naming of research outputs.
    *   Integration with Google Drive for storing final reports and any verification summaries.
    *   Client ownership of core LLM subscriptions, data, and the custom orchestration codebase.
*   **Out of Scope (for MVP):**
    *   Advanced UI/UX beyond basic voice input and feedback.
    *   Real-time progress updates during the 10-20 minute research phase (beyond basic "processing" indication).
    *   Complex, dynamic multi-agent collaboration beyond parallel or sequential research instances.
    *   User accounts or multi-user functionality.
    *   Advanced, interactive editing or annotation of research reports within the Dark Genie system itself (reports are for review in Google Drive).
    *   Direct fine-tuning of LLMs (utilizing pre-trained models via API).
    *   Integration with data sources other than web search (via Claude's tool) and the initial user prompt.

**4. User Stories (MVP)**

*   As a user, I want to speak a research query into the system so that I can initiate a deep research task without typing.
*   As a user, I want to specify the desired depth of research (e.g., "brief," "moderate," "in-depth," or a target number of research iterations) so that the output matches my needs.
*   As a user, I want the system to intelligently use Claude to find and synthesize comprehensive information related to my query from the web.
*   As a user, I want the initial research report generated by Claude to be automatically reviewed by another AI for accuracy and potential errors so I can have more confidence in the findings.
*   As a user, I want the final, verified research report to be automatically named based on my query and saved to my Google Drive so I can easily access and use it.

**5. Proposed Solution & High-Level Architecture**

Refer to the "How It Works" and "Technical Overview" sections from the previous README. The core components remain: Voice Input, Custom Orchestration Layer, Query Optimization LLM, Claude Research Engine, Evaluator LLM, and Google Drive Storage.

**6. Technical Rationale**

Refer to the "Technology Choices & Rationale" section from the previous README. The decision to use Anthropic's Claude API with its `web_search` tool for the primary research is key due to its current direct support for iterative, agentic web research via API.

**7. Tasks & Subtasks (Phased Approach for MVP Development)**

This is a proposed order, and some tasks can be parallelized.

**Phase 1: Core Research Engine & Basic Orchestration (Focus: Getting Claude to research and output)**

*   **Task 1.1: Setup Development Environment & Accounts**
    *   Subtask 1.1.1: Establish Anthropic Claude API access and billing.
    *   Subtask 1.1.2: Establish API access for the chosen "Query Optimization LLM" and "Evaluator LLM" (even if initially the same model as the research engine).
    *   Subtask 1.1.3: Setup Google Drive API access and authentication.
    *   Subtask 1.1.4: Initialize code repository (e.g., GitHub for the client).
*   **Task 1.2: Develop Core Claude Research Invocation**
    *   Subtask 1.2.1: Write script to take a text prompt and `max_uses` parameter.
    *   Subtask 1.2.2: Programmatically call Claude API with the `web_search` tool enabled.
    *   Subtask 1.2.3: Retrieve and store the raw output from Claude (text, citations).
    *   Subtask 1.2.4: Basic error handling for API calls.
*   **Task 1.3: Basic Output to Google Drive**
    *   Subtask 1.3.1: Write script to take a text file and upload it to a predefined Google Drive folder.
    *   Subtask 1.3.2: Implement basic file naming convention (e.g., query_timestamp.txt).

**Phase 2: Query Processing & Optimization (Focus: Improving research input)**

*   **Task 2.1: Develop Query Optimization Module**
    *   Subtask 2.1.1: Design prompts for the "Query Optimization LLM" to translate a user's natural language query into a more effective prompt for Claude's deep research.
    *   Subtask 2.1.2: Integrate API calls to this intermediary LLM.
*   **Task 2.2: Initial Voice Input Integration (Proof of Concept)**
    *   Subtask 2.2.1: Choose and integrate a basic Speech-to-Text (STT) solution (e.g., Python library, simple OS-level service).
    *   Subtask 2.2.2: Connect STT output to the Query Optimization Module.

**Phase 3: AI-Powered Verification Layer (Focus: Enhancing output reliability)**

*   **Task 3.1: Develop Evaluator LLM Module**
    *   Subtask 3.1.1: Design prompts for the "Evaluator LLM" to review Claude's research output for accuracy, hallucinations, and coherence. Define what "verification" means for the MVP (e.g., flagging uncertain claims, not exhaustive fact-checking).
    *   Subtask 3.1.2: Integrate API calls to the Evaluator LLM, feeding it Claude's output.
    *   Subtask 3.1.3: Determine how to capture and present the verification output (e.g., an appended section to the report, a separate summary file).
*   **Task 3.2: Integrate Verification into Workflow**
    *   Subtask 3.2.1: Orchestration layer to send Claude's research output to the Evaluator LLM.
    *   Subtask 3.2.2: Update Google Drive upload to include verification results.

**Phase 4: Workflow Orchestration & Parameterization (Focus: Tying it all together and adding user control)**

*   **Task 4.1: Develop Main Orchestration Script/Logic**
    *   Subtask 4.1.1: Sequence all modules: Voice Input -> Query Parsing -> Query Optimization -> Claude Research -> Evaluator LLM -> Google Drive Output.
    *   Subtask 4.1.2: Implement logic for handling user-defined depth parameters (translating "moderate depth" into a `max_uses` value for Claude, or a number of parallel instances).
    *   Subtask 4.1.3: Implement robust error handling and logging throughout the pipeline.
*   **Task 4.2: Refine File Naming and Folder Structure in Google Drive**
    *   Subtask 4.2.1: Allow for more dynamic naming based on query content.
    *   Subtask 4.2.2: Potentially create subfolders for different research tasks or dates.

**Phase 5: Testing & Refinement**

*   **Task 5.1: End-to-End MVP Testing**
    *   Subtask 5.1.1: Test with various voice queries and depth parameters.
    *   Subtask 5.1.2: Evaluate quality of research output and verification.
    *   Subtask 5.1.3: Test Google Drive integration.
*   **Task 5.2: Iteration & Bug Fixing**
    *   Subtask 5.2.1: Address any bugs or issues found during testing.
    *   Subtask 5.2.2: Refine prompts for Query Optimization and Evaluator LLMs based on test results.

**8. Release Criteria (MVP)**

*   User can successfully initiate a deep research task via a voice command.
*   The system can programmatically invoke Claude to conduct iterative web research.
*   The system can programmatically use an Evaluator LLM to review the initial output.
*   A named report (including any verification notes) is successfully saved to the user's Google Drive.
*   The core workflow is stable for a defined set of test cases.

**9. Success Metrics (MVP)**

*   Successful end-to-end execution of a research task from voice to Google Drive.
*   User satisfaction with the relevance and initial perceived quality of the research output (pre-manual review).
*   Perceived usefulness of the AI-driven verification step.
*   Stability and reliability of the system.

**10. Future Iterations (Post-MVP)**

*   More sophisticated UI/UX for query input and parameter adjustment.
*   Real-time feedback/progress indicators.
*   User ability to review and accept/reject verification findings.
*   Advanced "living document" features within Google Drive (e.g., AI-assisted updates to existing research).
*   Support for different types of research tasks or output formats.
*   Cost optimization strategies for API usage.